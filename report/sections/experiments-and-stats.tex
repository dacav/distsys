% ----------------------------------------------------------------------
\subsection{Tutorial: launching the application}
% ----------------------------------------------------------------------

Conventionally, an \Erlang\ \Keyword{application} has a target directory
in which binary object (\Path{.beam}) are pushed during compilation. This
directory is conventionally named \Path{ebin}, and the interpreter must be
aware of this position. So far, in order to launch the application, use
the following command line:
\begin{lstlisting}[language=bash]
$ erl -pa ebin
Erlang R14B03 (erts-5.8.4) [source] [64-bit] [smp:2:2] [rq:2]
[async-threads:0] [hipe] [kernel-poll:false]

Eshell V5.8.4  (abort with ^G)
1>
\end{lstlisting}

Before launching the application it may be necessary to re-compile the
modules:
\begin{lstlisting}[language=bash]
1> make:all([load]).
...
up_to_date
\end{lstlisting}

Then the application can be launched effectively:
\begin{lstlisting}[language=bash]
2> application:start(yuna).
Loading configuration...
Starting YUNA...
Starting main supervisor...
Starting peers pool...
Starting services...
ok
2012/1/4 3:30:59 <0.98.0>  Nodes started: N=500
2012/1/4 3:30:59 <0.98.0>  Preparing protocol...
2012/1/4 3:30:59 <0.98.0>  Nodes started. Letting them know each other...
...
\end{lstlisting}

After some time (depending on the execution) the process will stop:
\begin{lstlisting}[language=bash]
...
2012/1/4 3:32:4 <0.98.0>  Consensus (should) has been reached:
2012/1/4 3:32:4 <0.98.0>    Value false has been selected by 496 nodes
2012/1/4 3:32:4 <0.98.0>  Killing remaining processes...
2012/1/4 3:32:4 <0.98.0>  Terminating.
2012/1/4 3:32:4 <0.98.0>  Keeper finished.
\end{lstlisting}

This will produce the files containing the statistics. The name of the
file will depend, as explained in \Paragraph{subsub:ConfTechParams}, by a
prefix stored in the configuration variable. In this case (the prefix is
\emph{statfile} we have the following files:
\begin{lstlisting}[language=bash]
\$ ls statfile_*
statfile_decision_count.log  statfile_est_node_count.log statfile_events.log  statfile_node_count.log
\end{lstlisting}

From this files we can produce easily the graph of the execution by
feeding the \emph{gnuplot} software with the prefix and the configuration
I've put in the \Path{priv} directory:
\begin{lstlisting}[language=bash]
gnuplot -e 'prefix="statfile"' priv/plot_stats.gnuplot
\end{lstlisting}

This will produce a \emph{pdf} file named after the prefix, in this case
\Path{statfile.pdf}.

\ImageFW{pictures/run-default}{
    A run of the \emph{consensus} algorithm with the default settings.
    The green points (labelled \emph{est\_node\_count}) show the estimated
    number of nodes as provided by the \emph{Failure Detector} of the
    1\% of the nodes. The actual number of nodes is 500 (no nodes crashed
    during the experiment. The \emph{start protocol} and the \emph{end
    protocol} labels show respectively the startup instant of the
    \emph{consensus} protocol and the instant in which the nodes achieved
    the agreement.
}{fig:RunDefault}


% ----------------------------------------------------------------------
\subsection{Experiment: growing failure probability}
% ----------------------------------------------------------------------

For testing purposes I wrote a little \emph{perl} script
(\PathSrc{priv/probtest.pl}) which runs the application with different
(incremental) values for the \PCrash\ parameter. For each probability
value the script runs an experiment, feeding the following algorithm to
the interpreter:
\begin{enumerate}
\item   Start the application;
\item   Put an \emph{\Erlang\ monitor} on the \Keyword{keeper} process
        (in order to get informed when the keeper terminates);
\item   Send a synchronous message to the \Keyword{logging server} (in
        order to build a \emph{causality relation} and ensure all event
        messages are served before retrieving data).
\end{enumerate}
At this point the script retrieves the events (for instance the protocol
startup and stop time) for the terminated experiment. The collected data
is so far crunched, and displayed on a picture by \emph{gnuplot}
(\Figure{fig:ManyRuns}).

\ImageFW{pictures/many-runs}{
    Converging time for experiments. Experiments are run with incremental
    per-transmission failure probability ($x$ axis). As probability grows,
    the number of faulty nodes $f$ reaches $n/2$, thus the protocol is
    aborted. The pink line shows the average success time for each
    probability value. \See{\Paragraph{subsub:ManyRuns}}.
}{fig:ManyRuns}

I run the described experiments starting from a probability $\PCrash = 0$
(perfect run, no crashes), incrementing \PCrash\ of $0.005$ at each
experiment. I also defined the stop condition as $\PCrash < 1$ and
\emph{less than 5 consecutive fails}, so that the failure probability
remains in a reasonable interval.

I also achieved a little modification in my software, such that the
\Keyword{keeper} aborts the operation as the number of failing nodes $f$
overcomes $n/2$.

Apart from probability, the remaining configuration for this experiment
has been the following:
\begin{verbatim}
beaconwait 120
deliver_dist {random, uniform, []}
deliver_maxdel 1500
deliver_mindel 500
file_prefix statfile_default
keeper gfd_keeper
npeers 500
statpeers 0.01
tbeacon 500
tcleanup 20
tfail 10
tgossip 4
\end{verbatim}

\subsubsection{Considerations} \label{subsub:ManyRuns}

I expected the experiments to highlight a very strong susceptibility to
failure, even for small values of \PCrash. This somehow matches the
behavior shown in \Figure{fig:ManyRuns} (trivially, a greater process-wise
failure probability corresponds to a greater global failure probability).
In the experiment, however, the algorithm showed a surprising difference
with respect what I thought, which honestly I cannot explain without a
more precise analysis.

In my expectation I thought the failure would be primarily due to the
plentifulness of messages exchanged by the \emph{Failure Detector}, which
overcomes the traffic generated by the actual \emph{Consensus} algorithm.

Since a crashed process can no longer send data, I modelled system follows
a \emph{geometric distribution}:
\[
\Prob{\mbox{Crash at $k$-th transmission}} =
    \PCrash \cdot (1 - \PCrash)^{k - 1}
\]\[
\PrName{k} =
\Prob{\mbox{Being crashed after $k$ transmissions}} =
    \sum_{i = 1}^{k}\left( \PCrash \cdot (1 - \PCrash)^{i - 1} \right)
\]

As we can see in \Figure{fig:RunDefault}, a configuration having
$\SubsVar{T}{beacon} = 500 ms$ and $\SubsVar{T}{gossip} = 4$ yields a
pretty quick convergence of the \emph{Failure Detectors} to the actual
number of nodes. This however requires each process to send 2 messages per
second. As \VerbInline{beaconwait} is 120 (namely we wait 120 beacons
before starting the protocol):
\[
    \SubsVar{T}{wait}
        = 120 \beacon \cdot 0.500 \frac{\second}{\beacon} = 60 \second
\]

If the transmission rate is 2 messages per second, this means we send 120
packets \emph{before starting} the protocol, and if we relate this
behavior with the selected values of \PCrash we'll notice that the
protocol would hardly work with $\PCrash = 0.005$ (first step):

\begin{itemize}

\item   With $\PCrash = 0.005$ the probability of crashing, for every
        process, within the first minute is 0.54. This means that 274
        process over 500 would be crashed by then. This is already $f >
        \frac{n}{2}$;

\item   With $\PCrash = 0.01$ the expected number of alive process is only
        149;

\item   With $\PCrash = 0.015$ this value goes to 81.

\end{itemize}

I would like to investigate more on this, but honestly the time schedule
is tight, and I already gave much more time than what it's wise on this
experiment. I suspect however there's something wrong in my probabilistic
model.


% ----------------------------------------------------------------------
\subsection{Experiment: dead coordinator}
% ----------------------------------------------------------------------


