% ----------------------------------------------------------------------
\subsection{Tutorial: launching the application}
% ----------------------------------------------------------------------

Conventionally, an \Erlang\ \Keyword{application} has a target directory
in which binary object (\Path{.beam}) are pushed during compilation. This
directory is conventionally named \Path{ebin}, and the interpreter must be
aware of this position. So far, in order to launch the application, use
the following command line:
\begin{lstlisting}[language=bash]
$ erl -pa ebin
Erlang R14B03 (erts-5.8.4) [source] [64-bit] [smp:2:2] [rq:2]
[async-threads:0] [hipe] [kernel-poll:false]

Eshell V5.8.4  (abort with ^G)
1>
\end{lstlisting}

Before launching the application it may be necessary to re-compile the
modules:
\begin{lstlisting}[language=bash]
1> make:all([load]).
...
up_to_date
\end{lstlisting}

Then the application can be launched effectively:
\begin{lstlisting}[language=bash]
2> application:start(yuna).
Loading configuration...
Starting YUNA...
Starting main supervisor...
Starting peers pool...
Starting services...
ok
2012/1/4 3:30:59 <0.98.0>  Nodes started: N=500
2012/1/4 3:30:59 <0.98.0>  Preparing protocol...
2012/1/4 3:30:59 <0.98.0>  Nodes started. Letting them know each other...
...
\end{lstlisting}

After some time (depending on the execution) the process will stop:
\begin{lstlisting}[language=bash]
...
2012/1/4 3:32:4 <0.98.0>  Consensus (should) has been reached:
2012/1/4 3:32:4 <0.98.0>    Value false has been selected by 496 nodes
2012/1/4 3:32:4 <0.98.0>  Killing remaining processes...
2012/1/4 3:32:4 <0.98.0>  Terminating.
2012/1/4 3:32:4 <0.98.0>  Keeper finished.
\end{lstlisting}

This will produce the files containing the statistics. The name of the
file will depend, as explained in \Paragraph{subsub:ConfTechParams}, by a
prefix stored in the configuration variable. In this case (the prefix is
\emph{statfile} we have the following files:
\begin{lstlisting}[language=bash]
$ ls statfile_*
statfile_decision_count.log  statfile_est_node_count.log statfile_events.log  statfile_node_count.log
\end{lstlisting}

From this files we can produce easily the graph of the execution by
feeding the \emph{gnuplot} software with the prefix and the configuration
I've put in the \Path{priv} directory:
\begin{lstlisting}[language=bash]
$ gnuplot -e 'prefix="statfile"' priv/plot_stats.gnuplot
\end{lstlisting}

This will produce a \emph{pdf} file named after the prefix, in this case
\Path{statfile.pdf}. 

% ----------------------------------------------------------------------
\subsection{Experiment: ambush to the coordinator}
% ----------------------------------------------------------------------

For testing purposes I patched the program in order to allow an external
process (ideally the \Erlang\ shell) to tamper the system before the
protocol runs.

I added an external boolean parameter, \VerbInline{start\_direct}, which
can be setted to \Const{false} obtaining the system to wait for user
interaction \emph{before} starting the protocol.

I also added a few calls to the \Const{gfd_keeper} component (which is
the \Keyword{specific keeper} for the process):
\begin{itemize}
\item   \Func{persist}{1} \\
        If called with parameter \Const{true}, the \Keyword{keeper} is
        required to abort the protocol the number of failing nodes $f$
        overcomes $n/2$ (disable \emph{persistence}).
\item   \Func{launch_consensus}{0} \\
        Launch the protocol if it's not running yet
        (not running if \VerbInline{start\_direct} = \Const{false});
\item   \Func{schedule_killing}{1} \\
        Allows the external project to declare a list of identifiers to be
        killed. For instance \Const{schedule_killing([1])} will kill the
        coordinator just before starting the protocol.
\end{itemize}

Those functions are called by a small file which is directly passed (as
script) to an instance of the interpreter and run in batch mode. This file
executes the following sequence:
\begin{enumerate}
\item   Start the application;
\item   Tamper the execution by removing the \emph{persistence} of the
        \Keyword{keeper};
\item   Start the protocol;
\item   Put an \emph{\Erlang\ monitor} on the \Keyword{keeper} process
        (in order to get informed when the keeper terminates);
\item   Send a synchronous message to the \Keyword{logging server} (in
        order to build a \emph{causality relation} and ensure all event
        messages are served before retrieving data).
\end{enumerate}

Another version of the same file also schedules the killing of the
\emph{coordinator} before starting the protocol.

At this point I wrote a little \emph{perl} script
(\PathSrc{priv/probtest.pl}) which runs the application with different
(incremental) values for the \PCrash\ parameter. For each probability
value the script runs a certain number of experiments, than the same
experiments are run with the alternative version of the \emph{batch}, the
one which kills the \emph{coordinator}.

Finally the script retrieves the events (for instance the protocol
startup and stop time) for the terminated experiment. The collected data
is so far crunched, and displayed on a picture by \emph{gnuplot}
(\Figure{fig:ManyRuns}).

\ImageFW{pictures/many-runs}{
    Termination time for experiments. Experiments are run with incremental
    per-transmission failure probability ($x$ axis). As probability grows,
    the number of faulty nodes $f$ reaches $n/2$, thus the protocol is
    aborted. The pink line shows the average success time for each
    probability value. \See{\Paragraph{subsub:ManyRuns}}.
}{fig:ManyRuns}

I run the described experiments starting from a probability $\PCrash = 0$
(perfect run, no crashes), incrementing \PCrash\ of $0.005$ at each
experiment. I also defined the stop condition as $\PCrash < 1$ and
\emph{less than 5 consecutive fails}, so that the failure probability
remains in a reasonable interval.

Apart from probability, the remaining configuration for this experiment
has been the following:
\begin{verbatim}
beaconwait 120
deliver_dist {random, uniform, []}
deliver_maxdel 1500
deliver_mindel 500
file_prefix statfile_default
keeper gfd_keeper
npeers 500
statpeers 0.01
tbeacon 500
tcleanup 20
tfail 10
tgossip 4
\end{verbatim}

\subsubsection{Considerations} \label{subsub:ManyRuns}

In a predictable manner, experiments highlight a very strong
susceptibility to failure, even for small values of \PCrash:
since a crashed process can no longer send data, the crashing of the
processes follows a \emph{geometric distribution}:
\[
\Prob{\mbox{Crash at $k$-th transmission}} =
    \PCrash \cdot (1 - \PCrash)^{k - 1}
\]\[
\PrName{k} =
\Prob{\mbox{Being crashed after $k$ transmissions}} =
    \sum_{i = 1}^{k}\left( \PCrash \cdot (1 - \PCrash)^{i - 1} \right)
\]

This distribution grows pretty quickly, it's not weird that experiments
stop being meaningful after a threshold of 0.8, although a more in-depth
analysis should be made in order to determine the expected number of alive
processes in time.

\Image{pictures/final}{
    Termination time for experiments. Different behaviors when the
    coordinator has been killed before value proposal.
}{1}{fig:Final}

\Figure{fig:Final} shows the graph deriving from the described experiment.
We can see that, in general, the consensus protocol takes a small amount of
time to terminate (an average of $6904.69 ms$, with a peak of $62993 ms$ if
we set $\PCrash = 0$) but if we remove 

\ImageFW{pictures/run-default}{
    A run of the \emph{consensus} algorithm with the default settings.
    The green points (labelled \emph{est\_node\_count}) show the estimated
    number of nodes as provided by the \emph{Failure Detector} of the
    1\% of the nodes. The actual number of nodes is 500 (no nodes crashed
    during the experiment. The \emph{start protocol} and the \emph{end
    protocol} labels show respectively the startup instant of the
    \emph{consensus} protocol and the instant in which the nodes achieved
    the agreement.
}{fig:RunDefault}

\ImageFW{pictures/run-default-nocoord}{
    A run of the \emph{consensus} algorithm with the default settings.
    The green points (labelled \emph{est\_node\_count}) show the estimated
    number of nodes as provided by the \emph{Failure Detector} of the
    1\% of the nodes. The actual number of nodes is 500 (no nodes crashed
    during the experiment. The \emph{start protocol} and the \emph{end
    protocol} labels show respectively the startup instant of the
    \emph{consensus} protocol and the instant in which the nodes achieved
    the agreement.
}{fig:RunDefault}



\begin{figure}[tbh]
    \centering

    \subfigure[Protocol termination time for normal consensus]{
        \begin{tabular}{lrr}
        \toprule
        Probability & Avg time [ms] & Max time [ms] \\
        \midrule
        0       &  4918 & 10285 \\
        0.005   &  3261 & 4274 \\
        0.01    &  3032 & 4488 \\
        0.015   &  3394 & 7572 \\
        0.02    &  4112 & 6920 \\
        0.025   & 23031 & 62993 \\
        0.03    &  4723 & 9782 \\
        0.035   & 17863 & 32955 \\
        0.04    &  2327 & 2600 \\
        0.055   &  2385 & 2385 \\
        \bottomrule
        \end{tabular}
    }

    \subfigure[Protocol termination time for tampered consensus]{
        \begin{tabular}{lrr}
        \toprule
        Probability & Avg time [ms] & Max time [ms] \\
        \midrule
        0       &  74305 & 99863 \\
        0.005   & 116259 & 216993 \\
        0.01    &  78656 & 106305 \\
        0.015   &  68723 & 86209 \\
        0.02    &  56247 & 71610 \\
        0.025   &  35976 & 47802 \\
        0.03    &   5978 & 5978 \\
        \bottomrule
        \end{tabular}
    }

    \caption{\emph{Comparison between normal runs of the \emph{Consensus}
             algorithm and runs in which the coordinator in charge has
             been killed before value proposal.}}
    \label{tab:Results}
\end{figure}
